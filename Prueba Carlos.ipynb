{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad08c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbrales (train): 4.0 500.0\n",
      "Train size: (252187, 10)  Test size: (63047, 10)\n",
      "Distribución y_train: 0    0.560195\n",
      "1    0.439805\n",
      "Name: proportion, dtype: float64\n",
      "Distribución y_test : 0    0.563643\n",
      "1    0.436357\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0) Cargar datos\n",
    "df = pd.read_csv(\"./dataset_definitivo/dataset_definitivo.csv\")\n",
    "\n",
    "# 1) Limpieza mínima para evitar los problemas con el target \n",
    "\n",
    "df = df.dropna(subset=[\"NUM_INFRACCIONES\", \"CUANTIA\"]).copy()\n",
    "\n",
    "# 2) Split ANTES de crear el target \n",
    "X_full = df.copy()\n",
    "\n",
    "X_train_raw, X_test_raw = train_test_split(\n",
    "    X_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 3) Calcular umbrales SOLO en TRAIN\n",
    "p80_infracciones = X_train_raw[\"NUM_INFRACCIONES\"].quantile(0.80)\n",
    "p80_cuantia      = X_train_raw[\"CUANTIA\"].quantile(0.80)\n",
    "\n",
    "# 4) Crear el target en TRAIN y TEST usando UMBRALES DE TRAIN (no del test)\n",
    "def crear_target_alto_riesgo(df_in: pd.DataFrame,\n",
    "                             thr_infr: float,\n",
    "                             thr_cuant: float) -> pd.Series:\n",
    "    return np.where(\n",
    "        (df_in[\"NUM_INFRACCIONES\"] >= thr_infr) | (df_in[\"CUANTIA\"] >= thr_cuant),\n",
    "        1, 0\n",
    "    )\n",
    "\n",
    "y_train = crear_target_alto_riesgo(X_train_raw, p80_infracciones, p80_cuantia)\n",
    "y_test  = crear_target_alto_riesgo(X_test_raw,  p80_infracciones, p80_cuantia)\n",
    "\n",
    "# 5) Construir X eliminando lo que define el target (las dos de antes vaya)\n",
    "cols_leak = [\"NUM_INFRACCIONES\", \"CUANTIA\"]  \n",
    "X_train = X_train_raw.drop(columns=cols_leak).copy()\n",
    "X_test  = X_test_raw.drop(columns=cols_leak).copy()\n",
    "\n",
    "# 6) Checks rápidos para ver que sale mas o menos \n",
    "print(\"Umbrales (train):\", p80_infracciones, p80_cuantia)\n",
    "print(\"Train size:\", X_train.shape, \" Test size:\", X_test.shape)\n",
    "print(\"Distribución y_train:\", pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"Distribución y_test :\", pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2eb6c",
   "metadata": {},
   "source": [
    "## **En resumen**:\n",
    "\n",
    "### Definición del target \"ALTO_RIESGO\"\n",
    "\n",
    "Los umbrales utilizados corresponden al **percentil 80 (p80)** calculado únicamente sobre el conjunto de entrenamiento (`train`):\n",
    "\n",
    "- `p80_infracciones`: valor a partir del cual un conductor está en el 20% superior en número de infracciones.\n",
    "- `p80_cuantia`: valor a partir del cual un conductor está en el 20% superior en cuantía económica de sanciones.\n",
    "\n",
    "#### Regla de asignación del target\n",
    "\n",
    "Se asigna:\n",
    "\n",
    "- **1 (Alto Riesgo)** → Si el conductor cumple:\n",
    "  - `NUM_INFRACCIONES ≥ p80_infracciones`  \n",
    "  **o**\n",
    "  - `CUANTIA ≥ p80_cuantia`\n",
    "\n",
    "- **0 (Riesgo no alto)** → Si no cumple ninguna de las dos condiciones anteriores.\n",
    "\n",
    "Es decir, se considera incidencia grave cuando el conductor se sitúa en el 20% superior en al menos una de las dos variables.\n",
    "\n",
    "\n",
    "### Distribución de clases tras el split\n",
    "\n",
    "**Train**\n",
    "- Clase 0 → 56.02%\n",
    "- Clase 1 → 43.98%\n",
    "\n",
    "**Test**\n",
    "- Clase 0 → 56.36%\n",
    "- Clase 1 → 43.64%\n",
    "\n",
    "La proporción entre train y test es muy similar, lo que indica que el split mantiene coherencia en la distribución del target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47bdf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cortes severity_score (train): {'p60': 0.700863248303838, 'p75': 0.7583725568724795, 'p85': 0.8202433115109027, 'p95': 0.9023331892603504}\n",
      "Train size: (252187, 9)  Test size: (63047, 9)\n",
      "\n",
      "Distribución y_train (5 categorías):\n",
      "severity_score\n",
      "muy_baja    0.614584\n",
      "baja        0.141125\n",
      "media       0.094688\n",
      "alta        0.119741\n",
      "muy_alta    0.029863\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribución y_test (5 categorías):\n",
      "severity_score\n",
      "muy_baja    0.619522\n",
      "baja        0.140689\n",
      "media       0.092169\n",
      "alta        0.117563\n",
      "muy_alta    0.030057\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0) Cargar datos\n",
    "df = pd.read_csv(\"./dataset_definitivo/dataset_definitivo.csv\")\n",
    "\n",
    "# 1) Limpieza mínima\n",
    "needed = [\"NUM_INFRACCIONES\", \"CUANTIA\", \"PUNTOS\"]\n",
    "df = df.dropna(subset=needed).copy()\n",
    "\n",
    "# 2) Split *antes* de crear el target / umbrales\n",
    "X_full = df.copy()\n",
    "X_train_raw, X_test_raw = train_test_split(\n",
    "    X_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 3) Crear severity score basado en percentiles (rank pct) calculados SOLO con TRAIN\n",
    "# Básicamente para tener en cuenta por igual las 3 variables ya que trabajan en rangos diferentes\n",
    "# Como cuando escalabamos en regresión\n",
    "\n",
    "def add_severity_score(df_in: pd.DataFrame, ref_train: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df_in.copy()\n",
    "\n",
    "    def pct_rank_from_train(x: pd.Series, train_series: pd.Series) -> pd.Series:\n",
    "       \n",
    "        train_vals = np.sort(train_series.to_numpy())\n",
    "        return pd.Series(np.searchsorted(train_vals, x.to_numpy(), side=\"right\") / len(train_vals), index=x.index)\n",
    "\n",
    "    out[\"pr_infr\"]  = pct_rank_from_train(out[\"NUM_INFRACCIONES\"], ref_train[\"NUM_INFRACCIONES\"])\n",
    "    out[\"pr_cuant\"] = pct_rank_from_train(out[\"CUANTIA\"],          ref_train[\"CUANTIA\"])\n",
    "    out[\"pr_pts\"]   = pct_rank_from_train(out[\"PUNTOS\"],         ref_train[\"PUNTOS\"])\n",
    "\n",
    "    # pesos (me los ha dado chat gpt)\n",
    "    w_pts, w_infr, w_cuant = 0.40, 0.35, 0.25\n",
    "    out[\"severity_score\"] = (\n",
    "        w_pts  * out[\"pr_pts\"] +\n",
    "        w_infr * out[\"pr_infr\"] +\n",
    "        w_cuant* out[\"pr_cuant\"]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "train_scored = add_severity_score(X_train_raw, X_train_raw)\n",
    "test_scored  = add_severity_score(X_test_raw,  X_train_raw)\n",
    "\n",
    "# 4) Cortes en 5 categorías (definidos SOLO con TRAIN y aplicados a TEST)\n",
    "\n",
    "cuts = train_scored[\"severity_score\"].quantile([0.60, 0.75, 0.85, 0.95]).to_dict()\n",
    "c60, c75, c85, c95 = cuts[0.60], cuts[0.75], cuts[0.85], cuts[0.95]\n",
    "\n",
    "labels = [\"muy_baja\", \"baja\", \"media\", \"alta\", \"muy_alta\"]\n",
    "\n",
    "def to_5cats(score: pd.Series) -> pd.Categorical:\n",
    "    return pd.cut(\n",
    "        score,\n",
    "        bins=[-np.inf, c60, c75, c85, c95, np.inf],\n",
    "        labels=labels,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "y_train = to_5cats(train_scored[\"severity_score\"])\n",
    "y_test  = to_5cats(test_scored[\"severity_score\"])\n",
    "\n",
    "# 5) Construir X eliminando las columnas que definen el target para evitar leakage (las tres de antes otra vez vaya)\n",
    "cols_leak = [\"NUM_INFRACCIONES\", \"CUANTIA\", \"PUNTOS\"]\n",
    "X_train = X_train_raw.drop(columns=cols_leak).copy()\n",
    "X_test  = X_test_raw.drop(columns=cols_leak).copy()\n",
    "\n",
    "# 6) Salidas finales en esas 5 cat\n",
    "print(\"Cortes severity_score (train):\", {\"p60\": c60, \"p75\": c75, \"p85\": c85, \"p95\": c95})\n",
    "print(\"Train size:\", X_train.shape, \" Test size:\", X_test.shape)\n",
    "\n",
    "print(\"\\nDistribución y_train (5 categorías):\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True).reindex(labels).fillna(0))\n",
    "\n",
    "print(\"\\nDistribución y_test (5 categorías):\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True).reindex(labels).fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87232ab",
   "metadata": {},
   "source": [
    "## Construcción del `severity_score`\n",
    "\n",
    "El `severity_score` se construye como una combinación ponderada de tres variables:\n",
    "\n",
    "- `CUANTIA`\n",
    "- `PUNTOS`\n",
    "- `NUM_INFRACCIONES`\n",
    "\n",
    "Primero, cada variable se normaliza en rango 0–1 (para que sean comparables y adimensionales).\n",
    "\n",
    "Después, se aplica una mini-ecuación con pesos:\n",
    "\n",
    "severity_score = w1·CUANTIA_norm + w2·PUNTOS_norm + w3·NUM_INFRACCIONES_norm\n",
    "\n",
    "Donde:\n",
    "\n",
    "- w1, w2, w3 = pesos asignados según la importancia relativa que queremos dar a cada variable.\n",
    "- El resultado es un coeficiente continuo entre 0 y 1.\n",
    "\n",
    "\n",
    "## Conversión del score en categorías\n",
    "\n",
    "Una vez obtenido el `severity_score`, se clasifica usando percentiles del conjunto de entrenamiento:\n",
    "\n",
    "- ≤ p60 → muy_baja  \n",
    "- p60–p75 → baja  \n",
    "- p75–p85 → media  \n",
    "- p85–p95 → alta  \n",
    "- `>` p95 → muy_alta  \n",
    "\n",
    "## Cómo modificar el modelo\n",
    "\n",
    "Este enfoque es flexible:\n",
    "\n",
    "1. **Cambiar la importancia relativa** → Ajustando los pesos (w1, w2, w3).\n",
    "2. **Cambiar la distribución de categorías** → Modificando los percentiles de corte.\n",
    "\n",
    "De esta forma, podemos adaptar:\n",
    "- Qué variables influyen más en la severidad.\n",
    "- Qué proporción del dataset consideramos “riesgo alto” o “riesgo extremo”.\n",
    "\n",
    "Esto permite ajustar el modelo si la división inicial no refleja bien la realidad que queremos representar, eso ya lo veremos pero vamos que de momento lo veo bastante bien."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
